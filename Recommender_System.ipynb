{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommender_System.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/temiafeye/Colab-Projects/blob/master/Recommender_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OzwrM_lWOBNg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This recommender system is built using Restriced Boltzmann Machine (RBM)\n",
        "\n",
        "The data set is obtained from https://grouplens.org/datasets/movielens/\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "myklUw3lOOk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.parallel \n",
        "import torch.optim as optim \n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09TbodPjOsTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import the dataset \n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding ='latin-1')\n",
        "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding ='latin-1')\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding ='latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-LvrSFsxOwJ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#prepare training set and test set \n",
        "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
        "#convert training set to an array\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42m-ZJjPOzVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Getting the number of users and movies \n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:, 0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:, 1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkvOe3AKO2_4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#convert the data into an array with user in lines and movies in columns\n",
        "#the RBM expects this data structure with observations in line and features in columns\n",
        "def convert(data):\n",
        "    #create a list of list\n",
        "    new_data = []\n",
        "    for id_users in range(1,nb_users + 1):\n",
        "        #for one user\n",
        "        id_movies = data[:,1][data[:,0] == id_users]\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        #apply it to the major list\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "#convert the data into Torch Tensors \n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(training_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JuqNAIGgO4wX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#convert the ratings into binary ratings 1 (Liked) or 0 (Not Liked)\n",
        "training_set[training_set == 0] = -1\n",
        "training_set[training_set == 1] = 0\n",
        "training_set[training_set == 2] = 0\n",
        "training_set[training_set >= 3] = 1\n",
        "test_set[test_set == 0] = -1\n",
        "test_set[test_set == 1] = 0\n",
        "test_set[test_set == 2] = 0\n",
        "test_set[test_set >= 3] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EOZZa921O7dk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating the architecture of the neural network \n",
        "#a probabilistic graphical model \n",
        "class RBM():\n",
        "    def __init__(self, nv, nh):\n",
        "        self.W = torch.randn(nh, nv) #initializes a tensor nh, nv according to a normal distribution\n",
        "        #initialize the bias for hidden node\n",
        "        self.a = torch.randn(1, nh) #function expects a 2d data structure\n",
        "        self.b = torch.randn(1, nv) #bias for the visible node\n",
        "        \n",
        "    #the probabily of P(h)given vis nothing more than the sigmoid activation function\n",
        "    def sample_h(self, x):\n",
        "        #compute probability of h given v \n",
        "        wx = torch.mm(x, self.W.t())\n",
        "        activation = wx + self.a.expand_as(wx)\n",
        "        p_h_given_v = torch.sigmoid(activation) #activation function\n",
        "        return p_h_given_v, torch.bernoulli(p_h_given_v) #returns binary possibilities\n",
        "    \n",
        "    #compute visible node given hidden node \n",
        "    def sample_v(self, y):\n",
        "        #compute probability of v given h\n",
        "        wy = torch.mm(y, self.W)\n",
        "        activation = wy + self.b.expand_as(wy) #apply bias to each batch of the mini-batch \n",
        "        p_v_given_h = torch.sigmoid(activation) #activation function\n",
        "        return p_v_given_h, torch.bernoulli(p_v_given_h) #returns binary possibilities based on bernollis sampling \n",
        "    \n",
        "    #apply contrastive divergence with Gibbs sampling, used to implement Log-Likelihood Gradient\n",
        "    #since we have an energy function we are trying to minimize\n",
        "    #we need to maximize the log-likelihood of the training set\n",
        "    \n",
        "    def train(self, v0, vk, ph0, phK):\n",
        "         self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phK)\n",
        "         self.b += torch.sum((v0 - vk), 0)\n",
        "         self.a +=  torch.sum((p0 - phk), 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gE_-nsgVPCV-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create  RBM Object \n",
        "nv = len(training_set[0])\n",
        "nh = 100 #nh is arbitrarily chosen \n",
        "batch_size = 100 #batch_size also tunable \n",
        "rbm = RBM(nv,nh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzEhBXdqPF2W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training the RBM \n",
        "#Choose number of epoch \n",
        "nb_epoch = 10\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0 #initialize train loss, we set to zero initially \n",
        "    s = 0. #we intend on normalize the train loss, so we divide the train loss by this counter, dtype=float\n",
        "    for id_user in range(0, nb_users - batch_size, batch_size): #contains, start, stop, range\n",
        "        vk = training_set[id_user:id_user+batch_size] #iniital input entering into the gibbs chain\n",
        "        v0 = training_set[id_user:id_user+batch_size] #input is the same as ouput at the beginning\n",
        "        #third variable is ph0, probablity that the ratings of the movies initially equals 1\n",
        "        #given the real rating of the movie \n",
        "        ph0,_ = rbm.sample_h(v0) # ,_ indicates that we want just eh first element of the returned set\n",
        "        for k in range(10): #the number of range for the contrastive divergence \n",
        "            _,hk = rbm.sample_h(vk)\n",
        "            _,vk = rbm.sample_v(hk)\n",
        "            #to freeze the visible nodes that have -1 rating, not originally rated \n",
        "            vk[v0<0] = v0[v0<0] #taking the original -1 ratings\n",
        "        phk,_ = rbm.sample_h(vk)\n",
        "        rbm.train(v0, vk, ph0, phk)\n",
        "        #to measure the train loss\n",
        "        train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vK[v0 >= 0]))\n",
        "        #update the counter\n",
        "        s += 1.\n",
        "    #we include the print function at every epoch step, thus the print function \n",
        "    #is included in the for loop\n",
        "    print('epoch: ' +str(epoch)+' loss: '+str(train_loss/s))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}